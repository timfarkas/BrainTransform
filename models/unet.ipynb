{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn.functional import relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "\n",
    "\n",
    "class EEGtoMEGUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder \n",
    "        # First Level - Modified to preserve temporal dimension better\n",
    "        self.e11 = nn.Conv2d(1, 128, kernel_size=(77, 3), padding=(0, 1), stride=(1, 1))    \n",
    "        self.e12 = nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))         \n",
    "\n",
    "        # Second Level\n",
    "        self.e21 = nn.Conv2d(128, 256, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.e22 = nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))\n",
    "\n",
    "        # Third Level\n",
    "        self.e31 = nn.Conv2d(256, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.e32 = nn.Conv2d(512, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=(1, 2), stride=(1, 2))        \n",
    "\n",
    "        # Bridge - Adjusted sizes based on actual tensor shapes\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.bridge_mlp = nn.Sequential(\n",
    "            nn.Linear(17408, 4096),  # 17408 = 512 * 1 * 34\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 17408),\n",
    "        )\n",
    "        self.unflatten = nn.Unflatten(1, (512, 1, 34))\n",
    "\n",
    "        # Decoder\n",
    "        self.upconv1 = nn.ConvTranspose2d(512, 512, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.d11 = nn.Conv2d(1024, 512, kernel_size=(1, 3), padding=(0, 1))   \n",
    "        self.d12 = nn.Conv2d(512, 512, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.d21 = nn.Conv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))    \n",
    "        self.d22 = nn.Conv2d(256, 256, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=(1, 2), stride=(1, 2))\n",
    "        self.d31 = nn.Conv2d(256, 128, kernel_size=(1, 3), padding=(0, 1))    \n",
    "        self.d32 = nn.Conv2d(128, 128, kernel_size=(1, 3), padding=(0, 1))    \n",
    "\n",
    "        # Output layer\n",
    "        self.outconv = nn.Conv2d(128, 102, kernel_size=(1, 1))                   \n",
    "        \n",
    "        # Final upsampling to match target size\n",
    "        self.final_upsample = nn.Upsample(size=(1, 275), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape input: [batch, channels, time] -> [batch, 1, channels, time]\n",
    "        x = x.unsqueeze(1)\n",
    "        \n",
    "        # Encoder\n",
    "        xe11 = self.relu(self.e11(x))\n",
    "        xe12 = self.relu(self.e12(xe11))\n",
    "        xp1 = self.pool1(xe12)\n",
    "        xp1 = self.dropout(xp1)\n",
    "\n",
    "        xe21 = self.relu(self.e21(xp1))\n",
    "        xe22 = self.relu(self.e22(xe21))\n",
    "        xp2 = self.pool2(xe22)\n",
    "        xp2 = self.dropout(xp2)\n",
    "\n",
    "        xe31 = self.relu(self.e31(xp2))\n",
    "        xe32 = self.relu(self.e32(xe31))\n",
    "        xp3 = self.pool3(xe32)\n",
    "        xp3 = self.dropout(xp3)\n",
    "\n",
    "        # Bridge\n",
    "        xb = self.flatten(xp3)\n",
    "        xb = self.bridge_mlp(xb)\n",
    "        xb2 = self.unflatten(xb)\n",
    "        xb2 = self.dropout(xb2)\n",
    "\n",
    "\n",
    "        # Decoder with size matching\n",
    "        xu1 = self.upconv1(xb2)\n",
    "        xu1 = torch.nn.functional.pad(xu1, [0, xe32.size(-1) - xu1.size(-1), 0, 0])\n",
    "        xu11 = torch.cat([xu1, xe32], dim=1)\n",
    "        xd11 = self.relu(self.d11(xu11))\n",
    "        xd12 = self.relu(self.d12(xd11))\n",
    "        xd12 = self.dropout(xd12)\n",
    "\n",
    "        xu2 = self.upconv2(xd12)\n",
    "        xu2 = torch.nn.functional.pad(xu2, [0, xe22.size(-1) - xu2.size(-1), 0, 0])\n",
    "        xu22 = torch.cat([xu2, xe22], dim=1)\n",
    "        xd21 = self.relu(self.d21(xu22))\n",
    "        xd22 = self.relu(self.d22(xd21))\n",
    "        xd22 = self.dropout(xd22)\n",
    "\n",
    "        xu3 = self.upconv3(xd22)\n",
    "        xu3 = torch.nn.functional.pad(xu3, [0, xe12.size(-1) - xu3.size(-1), 0, 0])\n",
    "        xu33 = torch.cat([xu3, xe12], dim=1)\n",
    "        xd31 = self.relu(self.d31(xu33))\n",
    "        xd32 = self.relu(self.d32(xd31))\n",
    "\n",
    "        # Output layer\n",
    "        out = self.outconv(xd32)\n",
    "        \n",
    "        # Upsample to match target size\n",
    "        out = self.final_upsample(out)\n",
    "        \n",
    "        # Remove the singleton dimension\n",
    "        out = out.squeeze(2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 77, 275])\n",
      "After unsqueeze shape: torch.Size([2, 1, 77, 275])\n",
      "After e11 shape: torch.Size([2, 128, 1, 275])\n",
      "After e12 shape: torch.Size([2, 128, 1, 275])\n",
      "After pool1 shape: torch.Size([2, 128, 1, 137])\n",
      "Output shape: torch.Size([2, 102, 275])\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "batch_size = 2\n",
    "eeg_channels = 77  # Matches the first conv layer's kernel size\n",
    "time_points = 275  # This should give us the correct size after pooling operations\n",
    "\n",
    "test_data = torch.randn(batch_size, eeg_channels, time_points)\n",
    "print(f\"Input shape: {test_data.shape}\")\n",
    "\n",
    "# Initialize and test\n",
    "model = EEGtoMEGUNet()\n",
    "model.eval()\n",
    "\n",
    "# Let's add some debug prints to track the tensor shapes\n",
    "def print_shape(tensor, name):\n",
    "    print(f\"{name} shape: {tensor.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Add this inside your model's forward method temporarily\n",
    "    x = test_data.unsqueeze(1)\n",
    "    print_shape(x, \"After unsqueeze\")\n",
    "    \n",
    "    xe11 = model.relu(model.e11(x))\n",
    "    print_shape(xe11, \"After e11\")\n",
    "    \n",
    "    xe12 = model.relu(model.e12(xe11))\n",
    "    print_shape(xe12, \"After e12\")\n",
    "    \n",
    "    xp1 = model.pool1(xe12)\n",
    "    print_shape(xp1, \"After pool1\")\n",
    "    \n",
    "    # Continue with the rest of the forward pass\n",
    "    output = model(test_data)\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random array test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 77, 275])\n",
      "Output shape: torch.Size([2, 102, 275])\n"
     ]
    }
   ],
   "source": [
    "# Create test data\n",
    "batch_size = 2\n",
    "eeg_channels = 77\n",
    "time_points = 275\n",
    "\n",
    "test_data = torch.randn(batch_size, eeg_channels, time_points)\n",
    "print(f\"Input shape: {test_data.shape}\")\n",
    "\n",
    "# Initialize and test\n",
    "model = EEGtoMEGUNet()\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(test_data)\n",
    "    print(f\"Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synaptech_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
